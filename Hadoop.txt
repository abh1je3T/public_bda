###Hadoop Installation

sudo apt-get upgrade 
sudo apt-get update 
sudo apt-get install openjdk-8-jdk 
sudo addgroup hadoop 
sudo adduser --ingroup hadoop hduser 
sudo usermod -aG sudo hduser 
su hduser 
ssh-keygen -t rsa -P "" 
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys 

sudo nano /etc/sysctl.conf

net.ipv6.conf.all.disable_ipv6 = 1 
net.ipv6.conf.default.disable_ipv6 = 1 
net.ipv6.conf.lo.disable_ipv6 = 1 

cd /usr/local 
sudo wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz 
sudo tar xzf hadoop-3.3.0.tar.gz 
sudo mv hadoop-3.3.0 hadoop 
sudo chown -R hduser:hadoop hadoop 


sudo nano ~/.bashrc

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64 
unalias fs &> /dev/null 
alias fs="hadoop fs" 
unalias hls &> /dev/null 
alias hls="fs -ls" 
lzohead () { 
    hadoop fs -cat $1 | lzop -dc | head -1000 | less 
} 
export PATH=$PATH:$HADOOP_HOME/bin 
export PATH=$PATH:/usr/local/hadoop/sbin 


source ~/.bashrc
cd /usr/local/hadoop/etc/hadoop
sudo nano hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64

sudo mkdir -p /app/hadoop/tmp
chown hduser:hadoop /app/hadoop/tmp

cd /usr/local/hadoop/etc/hadoop
sudo nano core-site.xml
<property> 
  <name>hadoop.tmp.dir</name> 
  <value>/app/hadoop/tmp</value> 
  <description>A base for other temporary directories.</description> 
</property> 
<property> 
  <name>fs.default.name</name> 
  <value>hdfs://localhost:54310</value> 
  <description>The name of the default file system.  A URI whose 
  scheme and authority determine the FileSystem implementation.  The 
  uri's scheme determines the config property (fs.SCHEME.impl) naming 
  the FileSystem implementation class.  The uri's authority is used to 
  determine the host, port, etc. for a filesystem.</description> 
</property> 

sudo nano mapred-site.xml

<property> 
  <name>mapred.job.tracker</name> 
  <value>localhost:54311</value> 
  <description>The host and port that the MapReduce job tracker runs 
  at.  If "local", then jobs are run in-process as a single map 
  and reduce task. 
  </description> 
</property> 

sudo nano  hdfs-site.xml

<property> 
<name>dfs.replication</name> 
<value>1</value> 
<description>Default block replication. 
The actual number of replications can be specified when the file is created. 
The default is used if replication is not specified in create time. 
</description> 
property> 

hadoop namenode -format 


sudo apt remove openssh-server 
sudo apt install openssh-server 


sudo service ssh start 
ssh localhost

ssh-keygen -t rsa -P "" 
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys 

/usr/local/hadoop/sbin/start-all.sh 
jps

/usr/local/hadoop/sbin/stop-all.sh 





